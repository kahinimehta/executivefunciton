{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script audits the attachments stored on flywheel, either at the acquisition or session level.\n",
    "#Inputs:\n",
    "    #enrollment sheets pulled from axis, stored at afp://saturn/Coordinators/Protocols/TED_PROTOCOLS/EXECUTIVE_829744/2022_data_freeze/inputs/\n",
    "    #data from Flywheel acessed using the CLI \n",
    "#Outputs:\n",
    "    #Binary audit of attachments related to task data (.log, .json, .tsv) on Flywheel \n",
    "    #Binary audit of variability on Flywheel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flywheel\n",
    "import pandas as pd\n",
    "import re \n",
    "fw = flywheel.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EFProj=fw.projects.find_first('label=EFR01')\n",
    "#print(EFProj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through subs and get sessions\n",
    "subjects=EFProj.subjects()\n",
    "\n",
    "sessions=[]\n",
    "for s in subjects :\n",
    "    tempsessions=s.sessions()\n",
    "    sessions.extend(tempsessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in T1 enrollment\n",
    "#adjust file path as necessary -- currently assuming parent directory is saturn folder referenced in first block \n",
    "axis_t1=pd.read_csv('/axis_enroll_t1.csv',dtype=str)\n",
    "axis_t1=axis_t1.drop(columns=['scan_1_date'])\n",
    "#and T2\n",
    "axis_t2=pd.read_csv('/axis_enroll_t2.csv',dtype=str)\n",
    "axis_t2=axis_t2.drop(columns=['scan_2_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat for list of scan IDs \n",
    "t1_scan=axis_t1['scan_id_timepoint_1']\n",
    "t1_scan=t1_scan.tolist()\n",
    "t1_scan = [str(t) for t in t1_scan]\n",
    "#and again\n",
    "t2_scan=axis_t2['scan_id_t2']\n",
    "t2_scan=t2_scan.tolist()\n",
    "t2_scan = [str(t) for t in t2_scan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of all log files attached at the session level of flywheel \n",
    "log_files=[]\n",
    "\n",
    "for ses in sessions:\n",
    "    sesid=ses.label\n",
    "    #print(sesid)\n",
    "    acq=ses.acquisitions()\n",
    "    for a in acq:\n",
    "        if 'func_task-fracnoback_run-02' in a.label or 'ABCD_fMRI_frac-no-back-run1' in a.label:\n",
    "            EFFiles=a.files\n",
    "            EFNames=[x.name for x in EFFiles]\n",
    "            for y in EFNames:\n",
    "                if 'log' in y:\n",
    "                    log_files.append((sesid,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print((log_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_log=[]\n",
    "\n",
    "for t in t1_scan: \n",
    "    if t not in [l[0] for l in log_files]:\n",
    "        has_log.append((t,0))\n",
    "    else: \n",
    "        has_log.append((t,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('11012', 1), ('10960', 0), ('10959', 0), ('11138', 1), ('11000', 1), ('11003', 1), ('11140', 1), ('11115', 0), ('nan', 0), ('11209', 1), ('11126', 1), ('11112', 0), ('11316', 1), ('11143', 1), ('11142', 1), ('11249', 0), ('11127', 1), ('11290', 1), ('11291', 0), ('nan', 0), ('11208', 1), ('11206', 1), ('11159', 1), ('11242', 1), ('11207', 1), ('11160', 1), ('11161', 0), ('11146', 1), ('11132', 1), ('11184', 1), ('11147', 1), ('11221', 0), ('nan', 0), ('nan', 0), ('11180', 1), ('11210', 1), ('11211', 1), ('11205', 1), ('11233', 1), ('11217', 1), ('nan', 0), ('nan', 0), ('11238', 1), ('11276', 1), ('11302', 1), ('11289', 1), ('nan', 0), ('nan', 0), ('11264', 1), ('11324', 1), ('nan', 0), ('11320', 1), ('11370', 1), ('nan', 0), ('11321', 1), ('11319', 1), ('nan', 0), ('11332', 1), ('nan', 0), ('nan', 0), ('11334', 0), ('11365', 1), ('11375', 1), ('nan', 0), ('11351', 0), ('11359', 1), ('11404', 1), ('11405', 0), ('11366', 1), ('11392', 1), ('11396', 0), ('11376', 0), ('11381', 1), ('nan', 0), ('11399', 1), ('nan', 0), ('11397', 1), ('11385', 1), ('11386', 1), ('nan', 0), ('11388', 1), ('11387', 1), ('11417', 0), ('nan', 0), ('nan', 0), ('11440', 1), ('11441', 0), ('11416', 1), ('11433', 1), ('11443', 1), ('11419', 1), ('11436', 1), ('nan', 0), ('11452', 1), ('11453', 0), ('11438', 1), ('11448', 1), ('11451', 1), ('11455', 1), ('11460', 1), ('nan', 0), ('11475', 1), ('11468', 1), ('nan', 0), ('11148', 0), ('11577', 1), ('11473', 1), ('11465', 1), ('nan', 0), ('11640', 1), ('11574', 1), ('11488', 1), ('11720', 1), ('11492', 1), ('nan', 0), ('11541', 1), ('nan', 0), ('11576', 1), ('11798', 1), ('nan', 0), ('nan', 0), ('11625', 1), ('nan', 0), ('nan', 0), ('11651', 1), ('11702', 1), ('nan', 0), ('11664', 1), ('nan', 0), ('11685', 0), ('nan', 0), ('nan', 0), ('nan', 0), ('11794', 1), ('nan', 0), ('11829', 1), ('11723', 1), ('11780', 1), ('11824', 1), ('11825', 1), ('11750', 0), ('11779', 1), ('11755', 1), ('nan', 0), ('11751', 1), ('11793', 1), ('nan', 0), ('11781', 0), ('11848', 1), ('11849', 0), ('11875', 1), ('11862', 1), ('11853', 1), ('11885', 1), ('11886', 1), ('11960', 1), ('11981', 1), ('11955', 0), ('12097', 1), ('12046', 1), ('12057', 1), ('12080', 1), ('12081', 1), ('12106', 1), ('12107', 1), ('12508', 0), ('nan', 0), ('nan', 0), ('nan', 0), ('nan', 0), ('nan', 0), ('nan', 0), ('11533', 1)]\n"
     ]
    }
   ],
   "source": [
    "#print(has_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scanid</th>\n",
       "      <th>has_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11126</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11290</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11208</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11206</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11242</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>11751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>11793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>11781</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>11848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>11849</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>11875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>11862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>11853</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>11885</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>11886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>11960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>11981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>11955</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>12097</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>12046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>12057</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>12080</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>12081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>12106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>12107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>12508</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>11533</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    scanid  has_log\n",
       "0    11012        1\n",
       "1    10960        0\n",
       "2    10959        0\n",
       "3    11138        1\n",
       "4    11000        1\n",
       "5    11003        1\n",
       "6    11140        1\n",
       "7    11115        0\n",
       "8      nan        0\n",
       "9    11209        1\n",
       "10   11126        1\n",
       "11   11112        0\n",
       "12   11316        1\n",
       "13   11143        1\n",
       "14   11142        1\n",
       "15   11249        0\n",
       "16   11127        1\n",
       "17   11290        1\n",
       "18   11291        0\n",
       "19     nan        0\n",
       "20   11208        1\n",
       "21   11206        1\n",
       "22   11159        1\n",
       "23   11242        1\n",
       "24   11207        1\n",
       "25   11160        1\n",
       "26   11161        0\n",
       "27   11146        1\n",
       "28   11132        1\n",
       "29   11184        1\n",
       "..     ...      ...\n",
       "143    nan        0\n",
       "144  11751        1\n",
       "145  11793        1\n",
       "146    nan        0\n",
       "147  11781        0\n",
       "148  11848        1\n",
       "149  11849        0\n",
       "150  11875        1\n",
       "151  11862        1\n",
       "152  11853        1\n",
       "153  11885        1\n",
       "154  11886        1\n",
       "155  11960        1\n",
       "156  11981        1\n",
       "157  11955        0\n",
       "158  12097        1\n",
       "159  12046        1\n",
       "160  12057        1\n",
       "161  12080        1\n",
       "162  12081        1\n",
       "163  12106        1\n",
       "164  12107        1\n",
       "165  12508        0\n",
       "166    nan        0\n",
       "167    nan        0\n",
       "168    nan        0\n",
       "169    nan        0\n",
       "170    nan        0\n",
       "171    nan        0\n",
       "172  11533        1\n",
       "\n",
       "[173 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save list as a dataframe\n",
    "t1_audit = pd.DataFrame(has_log, columns =['scanid', 'has_log'])\n",
    "t1_audit = t1_audit.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now, let's check for various attachments at the session level\n",
    "import re\n",
    "\n",
    "# initializing empty lists\n",
    "has_tsv=[]\n",
    "has_json=[]\n",
    "has_variability = []\n",
    "has_asl=[]\n",
    "\n",
    "# getting the names of all the files \n",
    "\n",
    "EFnames = []\n",
    "for ii in sessions: # looping through each session\n",
    "    for jj in range(len(ii[\"files\"])): # looping through each file attached to each session\n",
    "        file_name = (ii[\"files\"][jj][\"name\"]) # i.e: accessing the files value within each session's dictionary --> accessing each file in the list of files --> accessing the 'name' value within each dictionary there\n",
    "        EFnames.append(file_name)\n",
    "\n",
    "has_json_list = []\n",
    "for ii in EFnames: # going through each file in the list\n",
    "    if \"bold_events.json\" in ii: # all files that have the events.json should also have the events.tsv\n",
    "        ii = ii.split('_',2) # split so that ses_XXX is isolated as element index 1 in a list\n",
    "        ii = ii[1] # index ses-*\n",
    "        has_json_list.append(ii)\n",
    "\n",
    "has_tsv_list = []\n",
    "for ii in EFnames: # going through each file in the list\n",
    "    if \"bold_events.tsv\" in ii: # all files that have the events.json should also have the events.tsv\n",
    "        ii = ii.split('_',2) # split so that ses_XXX is isolated as element index 1 in a list\n",
    "        ii = ii[1] # index ses-*\n",
    "        has_tsv_list.append(ii)\n",
    "\n",
    "# repeat for aslcontext and variability \n",
    "\n",
    "has_asl_list = []\n",
    "for ii in EFnames: \n",
    "    if \"aslcontext\" in ii: \n",
    "        ii = ii.split('_',2) \n",
    "        ii = ii[1] # index ses-*\n",
    "        has_asl_list.append(ii)\n",
    "        \n",
    "has_variability_list = []\n",
    "for ii in EFnames: \n",
    "    if \"variability\" in ii:\n",
    "        ii = ii.split('_',2) # variability.zip is formatted differently\n",
    "        ii = ii[1] \n",
    "        has_variability_list.append(ii)\n",
    "\n",
    "all_sessions_list = []\n",
    "\n",
    "for t in t1_scan: \n",
    "    all_sessions_list.append(t) # append each session number to the all_sessions_list\n",
    "\n",
    "# creating a list of all sessions, but appending \"ses-\" at the beginning of each as this is how the information will be pulled out from Flywheel    \n",
    "all_sessions_list_ses = []  \n",
    "for ses in all_sessions_list: \n",
    "    ses = \"ses-\" + str(ses)\n",
    "    all_sessions_list_ses.append(ses)\n",
    "# print (all_sessions_list_ses)\n",
    "\n",
    "#looping through, filling out binary audits! \n",
    "#check jsons\n",
    "for item in all_sessions_list_ses: \n",
    "    if item not in has_json_list:\n",
    "        has_json.append((item,0))\n",
    "    else: \n",
    "        has_json.append((item,1))\n",
    "#check events tsvs        \n",
    "for item in all_sessions_list_ses: \n",
    "    if item not in has_tsv_list:\n",
    "        has_tsv.append((item,0))\n",
    "    else: \n",
    "        has_tsv.append((item,1))\n",
    "#check asl context        \n",
    "for item in all_sessions_list_ses: \n",
    "    if item not in has_asl_list :\n",
    "        has_asl.append((item,0))\n",
    "    else: \n",
    "        has_asl.append((item,1))\n",
    "#check variability        \n",
    "for item in all_sessions_list: # variability does not have sub-*-ses-* format, so will not need ses- appended\n",
    "    if item not in has_variability_list:\n",
    "        has_variability.append((item,0))\n",
    "    else: \n",
    "        has_variability.append((item,1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick reformat to be able to merge! \n",
    "has_tsv_reformat=[]\n",
    "for x in has_tsv:\n",
    "    y=x[1]\n",
    "    #z=y.split('-')[0]\n",
    "    #print(z)\n",
    "    has_tsv_reformat.append(y)\n",
    "    \n",
    "has_json_reformat=[]\n",
    "for x in has_json:\n",
    "    y=x[1]\n",
    "    #z=y.split('-')[0]\n",
    "    #print(z)\n",
    "    has_json_reformat.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in t1 task data audit dataframe \n",
    "t1_audit['has_json'] = has_json_reformat\n",
    "t1_audit['has_tsv'] = has_tsv_reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an audit data frame for variability\n",
    "t1_var_audit = pd.DataFrame(has_variability, columns =['scanid', 'has_variability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save sheets to disk \n",
    "t1_audit.to_csv('/Users/krmurtha/Desktop/EF_DATA_FREEZE/audits/EF_T1_task_audit.csv', sep = ',', index=False)\n",
    "t1_var_audit.to_csv('/Users/krmurtha/Desktop/EF_DATA_FREEZE/audits/EF_T1_variabilty_audit.csv', sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and check for .log's in T2 scans \n",
    "has_log2=[]\n",
    "\n",
    "for t in t2_scan: \n",
    "    if t not in [l[0] for l in log_files]:\n",
    "        has_log2.append((t,0))\n",
    "    else: \n",
    "        has_log2.append((t,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_audit = pd.DataFrame(has_log2, columns =['scanid', 'has_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now, let's check for various attachments at the session level for T2 \n",
    "\n",
    "# initializing empty lists\n",
    "has_tsv2=[]\n",
    "has_json2=[]\n",
    "has_variability2 = []\n",
    "has_asl2=[]\n",
    "\n",
    "all_sessions_list2 = []\n",
    "\n",
    "for t in t2_scan: \n",
    "    all_sessions_list2.append(t) # append each session number to the all_sessions_list\n",
    "\n",
    "# creating a list of all sessions, but appending \"ses-\" at the beginning of each as this is how the information will be pulled out from Flywheel    \n",
    "all_sessions_list_ses2 = []  \n",
    "for ses in all_sessions_list2: \n",
    "    ses = \"ses-\" + str(ses)\n",
    "    all_sessions_list_ses2.append(ses)\n",
    "# print (all_sessions_list_ses)\n",
    "\n",
    "#looping through, filling out binary audits! \n",
    "#check jsons\n",
    "for item in all_sessions_list_ses2: \n",
    "    if item not in has_json_list:\n",
    "        has_json2.append((item,0))\n",
    "    else: \n",
    "        has_json2.append((item,1))\n",
    "#check events tsvs        \n",
    "for item in all_sessions_list_ses2: \n",
    "    if item not in has_tsv_list:\n",
    "        has_tsv2.append((item,0))\n",
    "    else: \n",
    "        has_tsv2.append((item,1))\n",
    "#check asl context        \n",
    "for item in all_sessions_list_ses2: \n",
    "    if item not in has_asl_list :\n",
    "        has_asl2.append((item,0))\n",
    "    else: \n",
    "        has_asl2.append((item,1))\n",
    "#check variability        \n",
    "for item in all_sessions_list2: # variability does not have sub-*-ses-* format, so will not need ses- appended\n",
    "    if item not in has_variability_list:\n",
    "        has_variability2.append((item,0))\n",
    "    else: \n",
    "        has_variability2.append((item,1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick reformat to be able to merge! \n",
    "has_tsv_reformat2=[]\n",
    "for x in has_tsv2:\n",
    "    y=x[1]\n",
    "    #z=y.split('-')[0]\n",
    "    #print(z)\n",
    "    has_tsv_reformat2.append(y)\n",
    "    \n",
    "has_json_reformat2=[]\n",
    "for x in has_json2:\n",
    "    y=x[1]\n",
    "    #z=y.split('-')[0]\n",
    "    #print(z)\n",
    "    has_json_reformat2.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_audit['has_json']=has_json_reformat2\n",
    "t2_audit['has_tsv']=has_tsv_reformat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_var_audit = pd.DataFrame(has_variability2, columns =['scanid', 'has_variability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_audit.to_csv('/Users/krmurtha/Desktop/EF_DATA_FREEZE/audits/EF_T2_task_audit.csv', sep = ',', index=False)\n",
    "t2_var_audit.to_csv('/Users/krmurtha/Desktop/EF_DATA_FREEZE/audits/EF_T2_variabilty_audit.csv', sep = ',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
