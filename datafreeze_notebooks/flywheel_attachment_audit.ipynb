{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script audits the attachments stored on flywheel, either at the acquisition or session level.\n",
    "#Inputs:\n",
    "    #enrollment sheets pulled from axis, stored at afp://saturn/Coordinators/Protocols/TED_PROTOCOLS/EXECUTIVE_829744/2022_data_freeze/\n",
    "    #data from Flywheel acessed using the CLI \n",
    "#Outputs:\n",
    "    #Binary audit of attachments related to task data (.log, .json, .tsv) on Flywheel \n",
    "    #Binary audit of variability on Flywheel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flywheel\n",
    "import pandas as pd\n",
    "import re \n",
    "fw = flywheel.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krmurtha/anaconda3/lib/python3.7/site-packages/flywheel/flywheel.py:6274: UserWarning: Client version 14.6.6 does not match server version 16.4.3. Please update your client version!\n",
      "  warnings.warn('Client version {} does not match server version {}. Please update your client version!'.format(SDK_VERSION, release_version))\n",
      "WARNING:Flywheel:Use \"pip install flywheel-sdk~=16.4.4\" to install a compatible version for this server\n"
     ]
    }
   ],
   "source": [
    "EFProj=fw.projects.find_first('label=EFR01')\n",
    "#print(EFProj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through subs and get sessions\n",
    "subjects=EFProj.subjects()\n",
    "\n",
    "sessions=[]\n",
    "for s in subjects :\n",
    "    tempsessions=s.sessions()\n",
    "    sessions.extend(tempsessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in T1 enrollment\n",
    "#adjust file path as necessary -- currently assuming parent directory is saturn folder referenced in first block \n",
    "axis_t1=pd.read_csv('/Volumes/Coordinators/Protocols/TED_PROTOCOLS/EXECUTIVE_829744/2022_data_freeze/inputs/axis_enroll_t1.csv',dtype=str)\n",
    "axis_t1=axis_t1.drop(columns=['scan_1_date'])\n",
    "#and T2\n",
    "axis_t2=pd.read_csv('/Volumes/Coordinators/Protocols/TED_PROTOCOLS/EXECUTIVE_829744/2022_data_freeze/inputs/axis_enroll_t2.csv',dtype=str)\n",
    "axis_t2=axis_t2.drop(columns=['scan_2_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat for list of scan IDs \n",
    "t1_scan=axis_t1['scan_id_timepoint_1']\n",
    "t1_scan=t1_scan.tolist()\n",
    "t1_scan = [str(t) for t in t1_scan]\n",
    "#and again\n",
    "t2_scan=axis_t2['scan_id_t2']\n",
    "t2_scan=t2_scan.tolist()\n",
    "t2_scan = [str(t) for t in t2_scan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of all log files attached at the session level of flywheel \n",
    "log_files=[]\n",
    "\n",
    "for ses in sessions:\n",
    "    sesid=ses.label\n",
    "    #print(sesid)\n",
    "    acq=ses.acquisitions()\n",
    "    for a in acq:\n",
    "        if 'func_task-fracnoback_run-02' in a.label or 'ABCD_fMRI_frac-no-back-run1' in a.label:\n",
    "            EFFiles=a.files\n",
    "            EFNames=[x.name for x in EFFiles]\n",
    "            for y in EFNames:\n",
    "                if 'log' in y:\n",
    "                    log_files.append((sesid,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_log=[]\n",
    "\n",
    "for t in t1_scan: \n",
    "    if t not in [l[0] for l in log_files]:\n",
    "        has_log.append((t,0))\n",
    "    else: \n",
    "        has_log.append((t,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save list as a dataframe\n",
    "t1_audit = pd.DataFrame(has_log, columns =['scanid', 'has_log'])\n",
    "t1_audit = t1_audit.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now, let's check for various attachments at the session level\n",
    "import re\n",
    "\n",
    "# initializing empty lists\n",
    "has_tsv=[]\n",
    "has_json=[]\n",
    "has_variability = []\n",
    "has_asl=[]\n",
    "\n",
    "# getting the names of all the files \n",
    "\n",
    "EFnames = []\n",
    "for ii in sessions: # looping through each session\n",
    "    for jj in range(len(ii[\"files\"])): # looping through each file attached to each session\n",
    "        file_name = (ii[\"files\"][jj][\"name\"]) # i.e: accessing the files value within each session's dictionary --> accessing each file in the list of files --> accessing the 'name' value within each dictionary there\n",
    "        EFnames.append(file_name)\n",
    "\n",
    "has_json_list = []\n",
    "for ii in EFnames: # going through each file in the list\n",
    "    if \"bold_events.json\" in ii: # all files that have the events.json should also have the events.tsv\n",
    "        ii = ii.split('_',2) # split so that ses_XXX is isolated as element index 1 in a list\n",
    "        ii = ii[1] # index ses-*\n",
    "        has_json_list.append(ii)\n",
    "\n",
    "has_tsv_list = []\n",
    "for ii in EFnames: # going through each file in the list\n",
    "    if \"bold_events.tsv\" in ii: # all files that have the events.json should also have the events.tsv\n",
    "        ii = ii.split('_',2) # split so that ses_XXX is isolated as element index 1 in a list\n",
    "        ii = ii[1] # index ses-*\n",
    "        has_tsv_list.append(ii)\n",
    "\n",
    "# repeat for aslcontext and variability \n",
    "\n",
    "has_asl_list = []\n",
    "for ii in EFnames: \n",
    "    if \"aslcontext\" in ii: \n",
    "        ii = ii.split('_',2) \n",
    "        ii = ii[1] # index ses-*\n",
    "        has_asl_list.append(ii)\n",
    "        \n",
    "has_variability_list = []\n",
    "for ii in EFnames: \n",
    "    if \"variability\" in ii:\n",
    "        ii = ii.split('_',2) # variability.zip is formatted differently\n",
    "        ii = ii[1] \n",
    "        has_variability_list.append(ii)\n",
    "\n",
    "all_sessions_list = []\n",
    "\n",
    "for t in t1_scan: \n",
    "    all_sessions_list.append(t) # append each session number to the all_sessions_list\n",
    "\n",
    "# creating a list of all sessions, but appending \"ses-\" at the beginning of each as this is how the information will be pulled out from Flywheel    \n",
    "all_sessions_list_ses = []  \n",
    "for ses in all_sessions_list: \n",
    "    ses = \"ses-\" + str(ses)\n",
    "    all_sessions_list_ses.append(ses)\n",
    "# print (all_sessions_list_ses)\n",
    "\n",
    "#looping through, filling out binary audits! \n",
    "#check jsons\n",
    "for item in all_sessions_list_ses: \n",
    "    if item not in has_json_list:\n",
    "        has_json.append((item,0))\n",
    "    else: \n",
    "        has_json.append((item,1))\n",
    "#check events tsvs        \n",
    "for item in all_sessions_list_ses: \n",
    "    if item not in has_tsv_list:\n",
    "        has_tsv.append((item,0))\n",
    "    else: \n",
    "        has_tsv.append((item,1))\n",
    "#check asl context        \n",
    "for item in all_sessions_list_ses: \n",
    "    if item not in has_asl_list :\n",
    "        has_asl.append((item,0))\n",
    "    else: \n",
    "        has_asl.append((item,1))\n",
    "#check variability        \n",
    "for item in all_sessions_list: # variability does not have sub-*-ses-* format, so will not need ses- appended\n",
    "    if item not in has_variability_list:\n",
    "        has_variability.append((item,0))\n",
    "    else: \n",
    "        has_variability.append((item,1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick reformat to be able to merge! \n",
    "has_tsv_reformat=[]\n",
    "for x in has_tsv:\n",
    "    y=x[1]\n",
    "    #z=y.split('-')[0]\n",
    "    #print(z)\n",
    "    has_tsv_reformat.append(y)\n",
    "    \n",
    "has_json_reformat=[]\n",
    "for x in has_json:\n",
    "    y=x[1]\n",
    "    #z=y.split('-')[0]\n",
    "    #print(z)\n",
    "    has_json_reformat.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in t1 task data audit dataframe \n",
    "t1_audit['has_json'] = has_json_reformat\n",
    "t1_audit['has_tsv'] = has_tsv_reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an audit data frame for variability\n",
    "t1_var_audit = pd.DataFrame(has_variability, columns =['scanid', 'has_variability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save sheets to disk \n",
    "t1_audit.to_csv('/Volumes/Coordinators/Protocols/TED_PROTOCOLS/EXECUTIVE_829744/2022_data_freeze/audits/EF_T1_task_audit.csv', sep = ',', index=False)\n",
    "t1_var_audit.to_csv('/Volumes/Coordinators/Protocols/TED_PROTOCOLS/EXECUTIVE_829744/2022_data_freeze/audits/EF_T1_variabilty_audit.csv', sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and check for .log's in T2 scans \n",
    "has_log2=[]\n",
    "\n",
    "for t in t2_scan: \n",
    "    if t not in [l[0] for l in log_files]:\n",
    "        has_log2.append((t,0))\n",
    "    else: \n",
    "        has_log2.append((t,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_audit = pd.DataFrame(has_log2, columns =['scanid', 'has_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now, let's check for various attachments at the session level for T2 \n",
    "\n",
    "# initializing empty lists\n",
    "has_tsv2=[]\n",
    "has_json2=[]\n",
    "has_variability2 = []\n",
    "has_asl2=[]\n",
    "\n",
    "all_sessions_list2 = []\n",
    "\n",
    "for t in t2_scan: \n",
    "    all_sessions_list2.append(t) # append each session number to the all_sessions_list\n",
    "\n",
    "# creating a list of all sessions, but appending \"ses-\" at the beginning of each as this is how the information will be pulled out from Flywheel    \n",
    "all_sessions_list_ses2 = []  \n",
    "for ses in all_sessions_list2: \n",
    "    ses = \"ses-\" + str(ses)\n",
    "    all_sessions_list_ses2.append(ses)\n",
    "# print (all_sessions_list_ses)\n",
    "\n",
    "#looping through, filling out binary audits! \n",
    "#check jsons\n",
    "for item in all_sessions_list_ses2: \n",
    "    if item not in has_json_list:\n",
    "        has_json2.append((item,0))\n",
    "    else: \n",
    "        has_json2.append((item,1))\n",
    "#check events tsvs        \n",
    "for item in all_sessions_list_ses2: \n",
    "    if item not in has_tsv_list:\n",
    "        has_tsv2.append((item,0))\n",
    "    else: \n",
    "        has_tsv2.append((item,1))\n",
    "#check asl context        \n",
    "for item in all_sessions_list_ses2: \n",
    "    if item not in has_asl_list :\n",
    "        has_asl2.append((item,0))\n",
    "    else: \n",
    "        has_asl2.append((item,1))\n",
    "#check variability        \n",
    "for item in all_sessions_list2: # variability does not have sub-*-ses-* format, so will not need ses- appended\n",
    "    if item not in has_variability_list:\n",
    "        has_variability2.append((item,0))\n",
    "    else: \n",
    "        has_variability2.append((item,1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick reformat to be able to merge! \n",
    "has_tsv_reformat2=[]\n",
    "for x in has_tsv2:\n",
    "    y=x[1]\n",
    "    #z=y.split('-')[0]\n",
    "    #print(z)\n",
    "    has_tsv_reformat2.append(y)\n",
    "    \n",
    "has_json_reformat2=[]\n",
    "for x in has_json2:\n",
    "    y=x[1]\n",
    "    #z=y.split('-')[0]\n",
    "    #print(z)\n",
    "    has_json_reformat2.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_audit['has_json']=has_json_reformat2\n",
    "t2_audit['has_tsv']=has_tsv_reformat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_var_audit = pd.DataFrame(has_variability2, columns =['scanid', 'has_variability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_audit.to_csv('/Volumes/Coordinators/Protocols/TED_PROTOCOLS/EXECUTIVE_829744/2022_data_freeze/audits/EF_T2_task_audit.csv', sep = ',', index=False)\n",
    "t2_var_audit.to_csv('/Volumes/Coordinators/Protocols/TED_PROTOCOLS/EXECUTIVE_829744/2022_data_freeze/audits/EF_T2_variabilty_audit.csv', sep = ',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
